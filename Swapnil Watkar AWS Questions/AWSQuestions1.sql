Case 1: Application hosted in an EBS-backed Amazon EC2 instance,we need to encrypts the data first before writing it to the disk for storage:
Solution: Use AWS Key Management Service (AWS KMS)it create and control the encryption keys used to encrypt your data.
Case 2: Application get data from verious ways in GB ,We want to aggregate all the data in the fastest way (less time).
Solution:Use Amazon S3 Transfer Acceleration, it transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects.
Case 3: Which EC2 instance will be the first one to be terminated by your Auto Scaling group across three Availability Zones
Solution: Choose the Availability Zone with the instances that use the oldest launch configuration.
Case 4: Which custom metricis not readily available in CloudWatch to monitor your EC2 instances for highly available architecture with ELB and EC2.
Solution:Memory utilization ,You need to prepare a custom metric using CloudWatch Monitoring Scripts which is written in Perl.
Case 5: Application with CloudFront want to provide access to only paying subscribers without having to change their current URLs. 
Solution:Use Signed Cookies to access the private files,send the required Set-Cookie headers to the viewer which will unlock the content only to them.
Case 6: Application use ECS Cluster and RDS in Multi-AZ Deployments,you have to closely monitor different processes or threads on a DB instance, including the percentage of the CPU bandwidth and total memory consumed by each process. 
Solution: Enhanced Monitoring on RDS gathers its metrics from an agent on the instance, not like CloudWatch which get the matrics from hypervisor (VM) of DB.
Case 7: Application uses S3 with sensitive data (PII,PHI) and we want to set compliance policies that verify sensitive data also alerted if sensitive data found.
Solution:Use Amazon Macie security service that helps you prevent data loss by automatically discovering,classifying,and protecting sensitive data stored in S3. 
Case 8: Application uses EC2,ELB and a MySQL RDS,you want RDS database can only be accessed using the profile credentials specific to your EC2 instances via an authentication token.
Solution:Use AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL.
Case 9: Multiple Applications are in VPC, We what to temporarily deny access from the specified IP addresses which is causing issue inside VPC.
Solution:To control the traffic coming in and out of your VPC network, you can use the network access control list (ACL).
Case 10: Applications uses combination of Standard and Scheduled Reserved EC2 instances why we should not use Spot EC2 instances as they are cheaper.
Sloution:Reserved Instances don't get interrupted unlike Spot instances,capacity reservations available on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term through Scheduled Reserved Instances.
Case 11: Application uses EC2 instance in a private subnet,we want to send data to db and s3 via private endpoints that don't pass through the public Internet.
Solution:Use VPC endpoint service that will allow you to use private IP addresses to access both DynamoDB and S3 without any exposure to the public internet.
Case 12: Application uses ECS Cluster,which uses the Fargate launch type.The database credentials should be supplied using environment variables and you have to ensure that the credentials are secure and that they cannot be viewed in plaintext on the cluster itself. 
Solution:Use the AWS Systems Manager Parameter Store to keep the database credentials and then encrypt them using AWS KMS.Also Create an IAM Role for your ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store.
Case 13:Application uses CloudFront(web service).Users taking time to login also getting HTTP 504 errors.we need to resolve the issue and optimize the system.
Solution:Use Lambda@Edge to execute the authentication process in AWS locations closer to the users.Also set up an origin group with two origins. primary  and  second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.
Case 14:Application user EC2 and  Application Load Balancer  ,What will happen if one of these EC2 instances failed the health checks.
Solution:The Application Load Balancer stops sending traffic to that instance.
Case 15:What are the features in AWS that can ensure data security for your confidential documents in Cloud.
Solution:If your data is stored in EBS Volumes,enable EBS Encryption and if it is stored on Amazon S3, enable client-side and server-side encryption.
Case 16:Company needs a persistent block storage volume for data backup and stored data as object, also after 30 days archiving of data to storage service.
Solution:Use Amazon Elastic Block Store (EBS) since it is high performance block storage and S3 to store your backup data and configure a lifecycle policy to transition your objects to Amazon S3 Glacier.
Case 17:The company requires a secure and durable logging solution that will track all of the activities of all AWS resources on all regions.
Solution:CloudTrail can be used for this case with multi-region trail enabled, however, it will only cover the activities of the regional services (EC2, S3, RDS etc.) and not for global services such as IAM, CloudFront, AWS WAF, and Route 53. In order to satisfy the requirement, you have to add the --include-global-service-events parameter in your AWS CLI command.
Case 18:A company plans to improve the security of its database, use VPC flow logs to monitor all the COPY and UNLOAD traffic of the Redshift cluster that moves in and out of the VPC.what is the best way.
Solution:By using Enhanced VPC Routing, These features are used to tightly manage the flow of data between your Amazon Redshift cluster and other resources.
Case 19:Company needs solution to incorporates single sign-on feature from corporate AD or LDAP directory and also restricts access for each individual user to a designated user folder in an S3 bucket? 
Solution:Setup a Federation proxy or an Identity provider,Setup an AWS Security Token Service to generate temporary tokens,Configure an IAM role and an IAM Policy to access the bucket.
Case 20:A Company uses Internet Information Services (IIS) for Windows Server plan to migrate the application and file share to AWS what solution is best.
Solution:Amazon FSx for Windows File Server provides fully managed Microsoft Windows file servers.
Case 21:A Company wants full control over the encryption of the created keys and also the ability to immediately remove the key material from AWS KMS.The solution should also be able to audit the key usage independently of AWS CloudTrail.
Solution:AWS KMS can help you integrate with other AWS services to encrypt the data that you store in these services and control access to the keys that decrypt it.To immediately remove the key material from AWS KMS, you can use a custom key store. Take note that each custom key store is associated with an AWS CloudHSM cluster in your AWS account. Therefore, when you create an AWS KMS CMK in a custom key store, AWS KMS generates and stores the non-extractable key material for the CMK in an AWS CloudHSM cluster that you own and manage.This is also suitable if you want to be able to audit the usage of all your keys independently of AWS KMS or AWS CloudTrail.
Case 22:A company is using Amazon S3 to store frequently accessed data. When an object is created or deleted, the S3 bucket will send an event notification to the Amazon SQS queue. A solutions architect needs to create a solution that will notify the development and operations team about the created or deleted objects.
Solution:Since you need to send the notification to the development and operations team, you can use a combination of Amazon SNS and SQS. By using the message fanout pattern, you can create a topic and use two Amazon SQS queues to subscribe to the topic. If Amazon SNS receives an event notification, it will publish the message to both subscribers.
Case 23:Your company will be announcing a new revolutionary product and it is expected that your web portal will receive a massive number of visitors all around the globe. How can you protect your backend systems and applications from traffic spikes?
Solution:Use Amazon API Gateway Throttling limits,Throttling limits can be set for standard rates and bursts.
Case 24:Design solution that centrally manages AWS resources. also allow them to procure AWS resources centrally and share resources such as AWS Transit Gateways, AWS License Manager configurations, or Amazon Route 53 Resolver rules across their various accounts. 
Solution:AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization ,Consolidate all of the company's accounts using AWS Organizations.
Case 25:An Application uses DynamoDB,There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email. what is suitable solution for this.
Solution:Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to SNS Topic that will notify the subscribers via email.
Case 26:A company is in the process of migrating their applications to AWS. One of their systems requires a database that can scale globally and handle frequent schema changes. The application should not have any downtime or performance issues whenever there is a schema change in the database. It should also provide a low latency response to high-traffic queries.
Solution:Use DynamoDB,Its schema flexibility lets DynamoDB store complex hierarchical data within a single item.Composite key design lets it store related items close together on the same table.
Case 27:A company needs to deploy at least 2 EC2 instances to support the normal workloads of its application and automatically scale up to 6 EC2 instances to handle the peak load. The architecture must be highly available and fault-tolerant as it is processing mission-critical workloads.
Solution:Create an Auto Scaling group of EC2 instances and set the minimum capacity to 4 and the maximum capacity to 6. Deploy 2 instances in Availability Zone A and another 2 instances in Availability Zone B.
Case 28:The application has a message broker service which uses industry standard messaging APIs and protocols that must be migrated as well, without rewriting the messaging code in your application.need to migrate to AWS.
Solution:Amazon MQ-It supports industry-standard APIs and protocols so you can switch from any standards-based message broker to Amazon MQ without rewriting the messaging code in your applications.
Case 29:A travel photo sharing website is using Amazon S3 to serve high-quality photos to visitors of your website. After a few days, you found out that there are other travel websites linking and using your photos. This resulted in financial losses for your business.
Solution:Remove Pulbilc Read on s3 bucket and use presigned url with expiry.
Case 30:Application uses ElastiCache for Redis Since the other Cloud Engineers in your department have access to your ElastiCache cluster, you have to secure the session data in the portal by requiring them to enter a password before they are granted permission to execute Redis commands. how to achive this.
Solution:Using Redis AUTH command can improve data security by requiring the user to enter a password before they are granted permission to execute Redis commands on a password-protected Redis server.--transit-encryption-enabled and --auth-token parameters enabled.
Case 31:An application is hosted in an AWS Fargate cluster that runs a batch job whenever an object is loaded on an Amazon S3 bucket. The minimum number of ECS Tasks is initially set to 1 to save on costs, and it will only increase the task count based on the new objects uploaded on the S3 bucket. Once processing is done, the bucket becomes empty and the ECS Task count should be back to 1.Which is the most suitable option to implement with the LEAST amount of effort?
Solution:Set up a CloudWatch Event rule to detect S3 object PUT operations and set the target to the ECS cluster with the increased number of tasks. Create another rule to detect S3 DELETE operations and set the target to the ECS Cluster with 1 as the Task count.
Case 32:An organization needs to provision a new Amazon EC2 instance with a persistent block storage volume to migrate data from its on-premises network to AWS. The required maximum performance for the storage volume is 64,000 IOPS.
Solution:The AWS Nitro System is the underlying platform for the latest generation of EC2 instances that enables AWS to innovate faster, further reduce the cost of the customers, and deliver added benefits like increased security and new instance types.Launch a Nitro-based EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS.
Case 33:A company has a hybrid cloud architecture that connects their on-premises data center and cloud infrastructure in AWS.They require a durable storage backup for their corporate documents stored on-premises and a local cache that provides low latency access to their recently accessed data to reduce data egress charges.The documents must be stored to and retrieved from AWS via the Server Message Block (SMB) protocol. These files must immediately be accessible within minutes for six months and archived for another decade to meet the data compliance. what is the best and most cost-effective approach to implement this.
Solution:Launch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.
Case 34:You needs to set up a relational database and come up with a disaster recovery plan to mitigate multi-region failure. The solution requires a Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute.
Solution:Aurora Global Database supports storage-based replication that has a latency of less than 1 second.It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.it provides An RPO of 1 second and an RTO of less than 1 minute.
Case 35:Issue in environment where the user data stored in the S3 bucket has been accidentally deleted by one of the Junior DevOps Engineers. you were instructed to improve the security and protection of your AWS resources.    
Solution:By using Versioning and enabling MFA (Multi-Factor Authentication) on s3 bucket.
Case 36:A new relational database is needed that autoscales capacity to meet the needs of the application's peak load and scales back down when the surge of activity is over. What is the MOST cost-effective and suitable database setup in case of transactional workloads
Solution:Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. An Aurora Serverless DB cluster is a DB cluster that automatically starts up, shuts down, and scales up or down its compute capacity based on your application's needs.
Case 37:A company has a cloud architecture that is composed of Linux and Windows EC2 instances that process high volumes of financial data 24 hours a day, 7 days a week. To ensure high availability of the systems, you needs to create a solution that allows them to monitor the memory and disk utilization metrics of all the instances.
Solution:Install the CloudWatch agent to all the EC2 instances that gathers the memory and disk utilization data.View the custom metrics in the Amazon CloudWatch console.
Case 38:A Forex trading platform, which frequently processes and stores global financial data every minute, is hosted in your on-premises data center and uses an Oracle database. Due to a recent cooling problem in their data center, the company urgently needs to migrate their infrastructure to AWS to improve the performance of their applications. As the Solutions Architect, you are responsible in ensuring that the database is properly migrated and should remain available in case of database server failure in the future.
Solution:Creating an Oracle database in RDS with Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads.Amazon RDS performs an automatic failover to the standby, so that you can resume database operations as soon as the failover is complete.
Case 39:A telecommunications company is planning to give AWS Console access to developers. Company policy mandates the use of identity federation and role-based access control. Currently, the roles are already assigned using groups in the corporate Active Directory. what services can provide developers access to the AWS console
Solution:it is best to use AWS Directory Service AD Connector for easier integration. In addition, since the roles are already assigned using groups in the corporate Active Directory, it would be better to also use IAM Roles.
Case 40:A company needs to design an online analytics application that uses Redshift Cluster for its data warehouse. Which of the following services allows them to monitor all API calls in Redshift instance and can also provide secured data for auditing and compliance purposes?
Solution:AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. 
Case 41:An application that records weather data every minute is deployed in a fleet of Spot EC2 instances and uses a MySQL RDS database instance. Currently, there is only one RDS instance running in one Availability Zone. You plan to improve the database to ensure high availability by synchronous data replication to another RDS instance.
Solution:When you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.
Case 42:A company hosted a web application in an Auto Scaling group of EC2 instances. The IT manager is concerned about the over-provisioning of the resources that can cause higher operating costs. you need to instructed to create a cost-effective solution without affecting the performance of the application.
Solution:With a target tracking scaling policy (step scaling policies), you can increase or decrease the current capacity of the group based on a target value for a specific metric. This policy will help resolve the over-provisioning of your resources.
Case 43:You have identified a series of DDoS attacks while monitoring your VPC. you are responsible for fortifying your current cloud infrastructure to protect the data of your clients.what is suitable solution to mitigate these kinds of attacks
Solution:For higher levels of protection against attacks targeting applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced.AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT).
Case 44:A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand EC2 instances which use Amazon Aurora as its database. Currently, the system stores the file documents that the users uploaded in one of the attached EBS Volumes. Your manager noticed that the system performance is quite slow and he has instructed you to improve the architecture of the system.In this scenario, what will you do to implement a scalable, high throughput POSIX-compliant file system?
Solution:Amazon Elastic File System (Amazon EFS) provides simple, scalable, elastic file storage for use with AWS Cloud services and on-premises resources.Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time, allowing Amazon EFS to provide a common data source for workloads and applications running on more than one Amazon EC2 instance.
Case 45:A pharmaceutical company has resources hosted on both their on-premises network and in AWS cloud. They want all of their Software Architects to access resources on both environments using their on-premises credentials, which is stored in Active Directory. 
Solution:Since the company is using Microsoft Active Directory which implements Security Assertion Markup Language (SAML), you can set up a SAML-Based Federation for API Access to your AWS cloud.
Case 46:A tech company has a CRM application hosted on an Auto Scaling group of On-Demand EC2 instances. The application is extensively used during office hours from 9 in the morning till 5 in the afternoon. Their users are complaining that the performance of the application is slow during the start of the day but then works normally after a couple of hours.  
Solution:Configuring a Scheduled scaling policy for the Auto Scaling group to launch new instances before the start of the day 
Case 47:how can you protect the backend systems of the platform from traffic spikes.
Solution:Amazon API Gateway provides throttling at multiple levels including global and by service call. 
Case 48:It is expected that the database will have high-throughput workloads performing small, random I/O operations.you are required to properly set up and launch the required resources in AWS. Which of the following is the most suitable EBS type to use for your database?
Solution:Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency.
Case 49:A popular mobile game uses CloudFront, Lambda, and DynamoDB for its backend services. The player data is persisted on a DynamoDB table and the static assets are distributed by CloudFront. However, there are a lot of complaints that saving and retrieving player information is taking a lot of time.
Solution:Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache that can reduce Amazon DynamoDB response times from milliseconds to microseconds, even at millions of requests per second.
Case 50:An application consists of multiple EC2 instances in private subnets in different availability zones. The application uses a single NAT Gateway for downloading software patches from the Internet to the instances. There is a requirement to protect the application from a single point of failure when the NAT Gateway encounters a failure or if its availability zone goes down.How should the Solutions Architect redesign the architecture to be more highly available and cost-effective
Solution:Create a NAT Gateway in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone.
Case 51:You are working for a large financial company as an IT consultant. Your role is to help their development team to build a highly available web application using stateless web servers. In this scenario, which AWS services are suitable for storing session state data? 
Solution:DynamoDB and ElastiCache are the correct answers. You can store session state data on both DynamoDB and ElastiCache. These AWS services provide high-performance storage of key-value pairs which can be used to build a highly available web application.
Case 52:You have a requirement to make sure that an On-Demand EC2 instance can only be accessed from this IP address (110.238.98.71) via an SSH connection. Which configuration below will satisfy this requirement?
Solution:110.238.98.71/32 Protocal TCP ,port 22.
Case 53:Considering that the Lambda function is storing sensitive database and API credentials, how can you secure this information to prevent other developers in your team, or anyone, from seeing these credentials in plain text? Select the best option that provides the maximum security
Solution:When you create or update Lambda functions that use environment variables, AWS Lambda encrypts them using the AWS Key Management Service. When your Lambda function is invoked, those values are decrypted and made available to the Lambda code. Creating your own key gives you more flexibility, including the ability to create, rotate, disable, and define access controls, and to audit the encryption keys used to protect your data.
Case 54:A newly hired Solutions Architect is assigned to manage a set of CloudFormation templates that is used in the company's cloud architecture in AWS. The Architect accessed the templates and tried to analyze the configured IAM policy for an S3 bucket.
    {
     "Version": "2012-10-17",
     "Statement": [
     {
     "Effect": "Allow",
     "Action": [
     "s3:Get*",
     "s3:List*"
     ],
     "Resource": "*"
     },
     {
     "Effect": "Allow",
     "Action": "s3:PutObject",
     "Resource": "arn:aws:s3:::tutorialsdojo/*"
     }
     ]
    }
What does the above IAM policy allow?
Solution:An IAM user with this IAM policy is allowed to read objects from all S3 buckets owned by the account.
- An IAM user with this IAM policy is allowed to write objects into the 'tutorialsdojo' S3 bucket.
- An IAM user with this IAM policy is allowed to read objects from the 'tutorialsdojo' S3 bucket.
Case 55:There are a lot of outages in the Availability Zone of your RDS database instance to the point that you have lost access to the database. What could you do to prevent losing access to your database in case that this event happens again?
Solution: Enabling Multi-AZ failover. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.
Case 56:A company plans to host a web application in an Auto Scaling group of Amazon EC2 instances. The application will be used globally by users to upload and store different types of files. Based on user trends, files that are older than 2 years are accessed less frequently. The Solutions Architect of the company needs to create a cost-effective and scalable solution to store the old files yet still provide durability and high availability.Which of the following approach can be used to fulfill this requirement.
Solution: - Use Amazon S3 and create a lifecycle policy that will move the objects to Amazon S3 Glacier after 2 years.- Use Amazon S3 and create a lifecycle policy that will move the objects to Amazon S3 Standard-IA after 2 years.
Case 57:An online medical system hosted in AWS stores sensitive Personally Identifiable Information (PII) of the users in an Amazon S3 bucket. Both the master keys and the unencrypted data should never be sent to AWS to comply with the strict compliance and regulatory requirements of the company.Which S3 encryption technique should the Architect use?
Solution:Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options:
- Use an AWS KMS-managed customer master key.- Use a client-side master key.
Case 58:An application hosted in EC2 consumes messages from an SQS queue and is integrated with SNS to send out an email to you once the process is complete. The Operations team received 5 orders but after a few hours, they saw 20 email notifications in their inbox.Which of the following could be the possible culprit for this issue?
Solution:Web applcation not deleting the message after processing.Always remember that the messages in the SQS queue will continue to exist even after the EC2 instance has processed it, until you delete that message.
Case 59:A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using DynamoDB. You are instructed to improve the database performance by distributing the workload evenly and using the provisioned throughput efficiently.Which of the following would you consider to implement for your DynamoDB table?
Solution: partition keys with high-cardinality attributes: which have a large number of distinct values for each item.The partition key portion of a table's primary key determines the logical partitions in which a table's data is stored. This in turn affects the underlying physical partitions. Provisioned I/O capacity for the table is divided evenly among these physical partitions. Therefore a partition key design that doesn't distribute I/O requests evenly can create "hot" partitions that result in throttling and use your provisioned I/O capacity inefficiently.
Case 60:A startup is using Amazon RDS to store data from a web application. Most of the time, the application has low user activity but it receives bursts of traffic within seconds whenever there is a new product announcement. The Solutions Architect needs to create a solution that will allow users around the globe to access the data using an API.What should the Solutions Architect do meet the above requirement.
Solution:Based on the given scenario, you need to create a solution that will satisfy the two requirements. The first requirement is to create a solution that will allow the users to access the data using an API. To implement this solution, you can use Amazon API Gateway. The second requirement is to handle the burst of traffic within seconds. You should use AWS Lambda in this scenario because Lambda functions can absorb reasonable bursts of traffic for approximately 15-30 minutes.
Case 61:A Solutions Architect is hosting a website in an Amazon S3 bucket named tutorialsdojo. The users load the website using the following URL: http://tutorialsdojo.s3-website-us-east-1.amazonaws.com and there is a new requirement to add a JavaScript on the webpages in order to make authenticated HTTP GET requests against the same bucket by using the Amazon S3 API endpoint (tutorialsdojo.s3.amazonaws.com). Upon testing, you noticed that the web browser blocks JavaScript from allowing those requests.Which of the following options is the MOST suitable solution that you should implement for this scenario.
Solution:Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.
Case 62:An AI-powered Forex trading application consumes thousands of data sets to train its machine learning model. The application’s workload requires a high-performance, parallel hot storage to process the training datasets concurrently. It also needs cost-effective cold storage to archive those datasets that yield low profit.Which of the following Amazon storage services should the developer use?
Solution:The question has two requirements:High-performance, parallel hot storage to process the training datasets concurrently.Cost-effective cold storage to keep the archived datasets that are accessed infrequently.In this case, we can use Amazon FSx For Lustre for the first requirement, as it provides a high-performance, parallel file system for hot data. On the second requirement, we can use Amazon S3 for storing the cold data. Amazon S3 supports a cold storage system via Amazon S3 Glacier / Glacier Deep Archive.
Case 63:An online shopping platform is hosted on an Auto Scaling group of Spot EC2 instances and uses Amazon Aurora PostgreSQL as its database. There is a requirement to optimize your database workloads in your cluster where you have to direct the write operations of the production traffic to your high-capacity instances and point the reporting queries sent by your internal staff to the low-capacity instances.Which is the most suitable configuration for your application as well as your Aurora database cluster to achieve this requirement?
Solution:Amazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don't have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren't available.Creating a custom endpoint in Aurora based on the specified criteria for the production traffic and another custom endpoint to handle the reporting queries
Case 64:In Route 53, which record types will you use to point the DNS name of the Application Load Balancer
Solution:Alias with a type "AAAA" record set and Alias with a type "A" record set are correct.
Case 65:








